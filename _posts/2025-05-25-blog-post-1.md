---
title: 'Vaswani et al. (2017) - Attention is All You Need'
date: 2025-05-25
permalink: /posts/2025/05/blog-post-1/
tags:
  - ML
  - DL
  - NLP
  - Transformer
  - Attention
---

## 1. Introduction

Traditional sequence models like RNNs and LSTMs process sequences one token at a time. That sequential nature means you cannot parallelise them well, and long-range dependencies tend to get diluted.

To address this, the authors propose a new architecture called the Transformer, which completely eliminates recurrence. Instead of sequential dependencies, the Transformer uses attention mechanisms only to model relationships between tokens, allowing for much faster and more parallel computation.

Attention is a way for the model to focus on relevant parts of the input when generating each part of the output. Instead of treating all input tokens equally, attention lets the model weigh them differently depending on the context. Without attention, important dependencies — especially long-range ones — might be lost as information flows step by step through a recurrent model.

## 2. Background

Previous attempts to reduce sequential computation include convolution-based models like ByteNet, ConvS2S, and the Extended Neural GPU. These models improve parallelism but still have trouble modeling long-range dependencies efficiently, since the number of operations needed to connect distant tokens grows with distance.

The self-attention mechanism introduced in the Transformer solves this by allowing each token to attend to all other tokens simultaneously, regardless of their distance. This enables the model to capture long-range dependencies in constant time, and to do so in a way that is fully parallelisable.

## 3. Model Architecture

<p align="center">
  <img src="/images/post/post-1-1.png" alt="transformer" width="60%">
</p>

### 3.1. Encoder and Decoder Stacks

The encoder takes an input sequence  
\\[
  \mathbf{x} = (x_1, x_2, \dots, x_n)
\\]  
and processes it into a sequence of continuous vector representations  
\\[
  \mathbf{z} = (z_1, z_2, \dots, z_n),
\\]  
which capture the meaning and context of the input.

The decoder takes generates an output sequence  
\\[
  \mathbf{y} = (y_1, y_2, \dots, y_m)
\\]  
one token at a time, using the encoder output and its own previous outputs to ensure the output is fluent and grammatically consistent. Decoder uses masked attention to prevent access to future tokens.

Each encoder layer consists of
1. A multi-head self-attention mechanism.
2. A position-wise feed-forward network.

Each decoder layer has
1. A masked multi-head self-attention layer.
2. An encoder-decoder attention layer.
3. A feed-forward network**.

Each sublayer is wrapped with a residual connection and layer normalisation.

### 3.2. Attention

<p align="center">
  <img src="/images/post/post-1-2.png" alt="transformer" width="60%">
</p>

The core idea of attetion is simple but powerful: not all input words matter equally when generating an output. Attention allows each token to focus on the relevant tokens.

Start with an input sequence matrix \\( X \in \mathbb{R}^{n \times d_\text{model}} \\), where
- \\( n \\): sequence length
- \\( d_\text{model} \\): model dimension
learn three projection matrices or weights
- \\( W^Q, W^K, W^V \in \mathbb{R}^{d_\text{model} \times d_k} \\)

Think of projection matrices as different lenses through which the model views the input. Each lens distorts the input in a different way — one to ask questions, one to evaluate importance, and one to extract useful content. Through training, the model learns the best lenses to use for each of these roles.

Each token generates a query, key, and value vector.
\\[
  Q = XW^Q, \quad K = XW^K, \quad V = XW^V
\\]

The attention output is
\\[
  \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right)V
\\]
where
- \\( QK^\top \\): Similarity between tokens
- \\( \sqrt{d_k} \\): Scales scores to stabilise softmax
- \\( \text{softmax} \\): Converts to attention weights

The output gives the weighted sum of values — the attended output.

Think of Q as a question, K as an index of books and V as the actual content of those books. One is trying to answer a question — one look through all the indices (K) to find the best match, and then read the content (V) accordingly. Each token is essentially asking a question about the sequence, and attention figures out which tokens have the answers.

Single attention head has limited capacity to capture multiple dependencies. Hence, the author uses multi-head attention, which allows learning diverse relations in parallel, using different projection matrices for each head.

Split \\(d_{\text{model}}\\) into \\(h\\) heads, each of dimension \\(d_{k} = \frac{d_{\text{model}}}{h}\\).

Each head has its own parameters
\\[
  Q_i = XW_i^Q, \quad K_i = XW_i^K, \quad V_i = XW_i^V
\\]

Then
\\[
  \text{head}_i = \text{Attention}(Q_i, K_i, V_i)
\\]

All the heads are concatenated and projected
\\[
  \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
\\]

where \\( W^O \in \mathbb{R}^{hd_v \times d_\text{model}} \\)

Each head captures multiple types of relationships (syntax, semantics, position) and attends to different positions and dependencies.

### 3.3.  Position-wise Feed-Forward Networks

Each encoder and decoder layer includes a fully connected feed-forward network.
\\[
  \text{FFN}(x) = max(0, wX_{1} + b_{1})W_{2} + b_{2}
\\]

This is equivalent to two 1-D convolutions with kernel size 1. 

### 3.4. Embeddings and Softmax

Input and output tokens are mapped to \\(d_{\text{model}}\\)-dimensional vectors via learned embeddings. A shared weight matrix is used for input embeddings, output embeddings, and pre-softmax linear transformation. To preserve variance, weights are scaled by \\(\sqrt{d_{\text{model}}}\\).
Output prediction is done via softmax over the vocabulary.
\\[
  P(\mathbf{y_{i}}) = \text{softmax}(z_{i}W^{T})
\\]

### 3.5. Positional Encoding

Self-attention has no sense of order. To inject positional information, the model adds sinusoidal positional encodings to each input embedding.

\\[
  \text{PE}(pos, 2i) = \sin\left( \frac{pos}{10000^{2i/d}} \right) </br>
  \text{PE}(pos, 2i+1) = \cos\left( \frac{pos}{10000^{2i/d}} \right)
\\]

These encodings allow the model to reason about relative and absolute positions. They generalise well to sequences longer than those seen during training.

## 4. Why Self Attention

| Layer Type | Complexity | Sequential Ops | Max Path Length |
|------------|------------|----------------|-----------------|
| Self-Attn  | \\(O(n^2 \cdot d)\\) | \\(O(1)\\) | \\(O(1)\\) |
| RNN        | \\(O(n \cdot d^2)\\) | \\(O(n)\\) | \\(O(n)\\) |
| Conv       | \\(O(k \cdot n \cdot d^2)\\) | \\(O(1)\\) | \\(O(\log_k n)\\) |

## 5. Training

- Loss: Cross-entropy with label smoothing (\\( \epsilon = 0.1 \\))
- Optimiser: Adam with \\( \beta_1 = 0.9, \beta_2 = 0.98 \\)
- Learning rate schedule
  \\[
    \text{lrate} = d^{-0.5} \cdot \min(\text{step}^{-0.5}, \text{step} \cdot \text{warmup}^{-1.5})
  \\]
- Dropout and layer norm applied throughout
- Byte-Pair Encoding (BPE) used for tokenisation (vocab ~32K)

## 6. Results

| Model              | BLEU (En→De) | BLEU (En→Fr) |
|--------------------|--------------|--------------|
| GNMT + RL          | 24.6         | 39.0         |
| ConvS2S            | 25.2         | 40.5         |
| Transformer (base) | **27.3**     | **38.1**     |
| Transformer (big)  | **28.4**     | **41.8**     |

Beam search is used with a length penalty \\( \alpha = 0.6 \\). The model also performs well on parsing (92.7 F1), showing its generalisation power.

## 7. Conclusion

The Transformer introduces a simple yet powerful architecture that outperforms previous models while being faster and more parallelisable. It’s entirely based on attention, with no recurrence or convolution. The ideas from this paper now underpin many modern NLP models including BERT, GPT, and T5 — attention really is all you need.